{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ca8c805",
   "metadata": {
    "papermill": {
     "duration": 0.004704,
     "end_time": "2022-07-13T18:55:52.474790",
     "exception": false,
     "start_time": "2022-07-13T18:55:52.470086",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Transfer Learning\n",
    "\n",
    "## Introduction\n",
    "Transfer learning is a machine learning technique that leverages knowledge gained from one task to improve the performance of a related but different task. In traditional machine learning, models are trained for a specific task from scratch using a labeled dataset. Transfer learning, on the other hand, allows a pre-trained model, often developed on a large dataset for a particular task, to be adapted for a new, related task with a smaller dataset.\n",
    "\n",
    "The idea behind transfer learning is rooted in the notion that the knowledge acquired by a model in solving one problem can be valuable for solving a different, yet related, problem. This approach is particularly beneficial in situations where labeled data for the target task is limited or scarce.\n",
    "\n",
    "\n",
    "This notebook will guide you through the process of adapting the pre-trained model for a new task: predicting whether an image is a rural or urban by working with [this dataset](https://www.kaggle.com/datasets/dansbecker/urban-and-rural-photos). You will engage with the specific dataset associated with this task.\n",
    "\n",
    "The starting point is a pre-trained model on [ImageNet](https://en.wikipedia.org/wiki/ImageNet), an extensive dataset containing over 14 million images across thousands of categories. Keras provides access to various models (see [here](https://keras.io/api/applications/)) pre-trained on ImageNet, including ResNet, Xception and InceptionV3 etc.\n",
    "\n",
    "The initial layers of a deep learning model are adept at identifying simple shapes, while the subsequent layers excel at recognizing intricate visual patterns like roads, buildings, windows, and open fields. For our new application, these later layers prove valuable.\n",
    "\n",
    "The very last layer of the pre-trained model is responsible for making predictions. To tailor it to our needs, we'll substitute this final layer with a dense layer featuring two nodes. One node gauges the urban aspect of the photo, while the other assesses its rural aspect. Theoretically, any node in the last prediction layer may contribute information about how urban an image is. Therefore, the measure of urbanism can be influenced by all nodes in this layer. Similarly, each node's information may impact our measure of how rural the photo appears.\n",
    "\n",
    "Given the multitude of connections, we will utilize training data to discern which nodes indicate an image is urban, which suggest it is rural, and which nodes are inconsequential. Essentially, we are training the last layer of the model using labeled photos categorized as either rural or urban.\n",
    "\n",
    "Note: While classifying into only two categories would require only one node in the output, we have retained two separate nodes. This approach, with a distinct node for each potential category in the output layer, facilitates a seamless transition when predicting with more than two categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edddb66b",
   "metadata": {
    "papermill": {
     "duration": 0.003349,
     "end_time": "2022-07-13T18:55:52.482315",
     "exception": false,
     "start_time": "2022-07-13T18:55:52.478966",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Specify the Model\n",
    "\n",
    "In this application, we aim to classify photos into two categories or classes: urban and rural. We designate this as num_classes.\n",
    "\n",
    "To construct the model, we initiate a sequential model that allows us to progressively add layers. Initially, we incorporate a pre-trained ResNet model. The parameter `include_top=False` is used to specify the exclusion of the last layer of the ResNet model responsible for predictions. Additionally, we utilize a file without weights for that particular layer.\n",
    "\n",
    "The argument `pooling='avg'` indicates that if there are extra channels in our tensor after this step, we want to condense them into a 1D tensor by averaging. At this point, we have a pre-trained model creating the layer depicted in the graphic. We then introduce a `Dense` layer for making predictions, specifying the number of nodes equal to the number of classes. The softmax function is applied to generate probabilities.\n",
    "\n",
    "Lastly, we instruct TensorFlow not to train the initial layer of the sequential model, which consists of the ResNet50 layers. This is because the model has already undergone pre-training with the ImageNet data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8af3539",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-07-13T18:55:52.492155Z",
     "iopub.status.busy": "2022-07-13T18:55:52.491511Z",
     "iopub.status.idle": "2022-07-13T18:56:01.864345Z",
     "shell.execute_reply": "2022-07-13T18:56:01.863090Z"
    },
    "papermill": {
     "duration": 9.381297,
     "end_time": "2022-07-13T18:56:01.867268",
     "exception": false,
     "start_time": "2022-07-13T18:55:52.485971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set random seed / make reproducible\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "seed = 123\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8ad9cca",
   "metadata": {
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2022-07-13T18:56:01.877637Z",
     "iopub.status.busy": "2022-07-13T18:56:01.876425Z",
     "iopub.status.idle": "2022-07-13T18:56:06.582132Z",
     "shell.execute_reply": "2022-07-13T18:56:06.579894Z"
    },
    "papermill": {
     "duration": 4.714316,
     "end_time": "2022-07-13T18:56:06.585504",
     "exception": false,
     "start_time": "2022-07-13T18:56:01.871188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def create_model_from_resnet():\n",
    "    num_classes = 2\n",
    "    resnet_weights_path = './input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "    my_new_model = Sequential()\n",
    "    my_new_model.add(ResNet50(include_top=False, pooling='avg', weights=resnet_weights_path))\n",
    "    my_new_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Say not to train first layer (ResNet) model. It is already trained\n",
    "    my_new_model.layers[0].trainable = False\n",
    "    return my_new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15c8066d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 2048)              23587712  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 4098      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23591810 (90.00 MB)\n",
      "Trainable params: 4098 (16.01 KB)\n",
      "Non-trainable params: 23587712 (89.98 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_new_model = create_model_from_resnet()\n",
    "my_new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31205647",
   "metadata": {
    "papermill": {
     "duration": 0.003653,
     "end_time": "2022-07-13T18:56:06.593426",
     "exception": false,
     "start_time": "2022-07-13T18:56:06.589773",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Compile the Model\n",
    "\n",
    "The compile command in TensorFlow instructs how to adjust the connections in the final layer of the network during training.\n",
    "\n",
    "For the measure of loss or inaccuracy that we aim to minimize, we specify `categorical_crossentropy`. If you are acquainted with log-loss, this term is synonymous.\n",
    "\n",
    "To minimize the categorical cross-entropy loss, we employ an algorithm called stochastic gradient descent (SGD).\n",
    "\n",
    "Additionally, we request the code to report the accuracy metric, which represents the fraction of correct predictions. This metric is more intuitive than categorical cross-entropy scores, making it beneficial to display and assess the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe449ea0",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-07-13T18:56:06.603870Z",
     "iopub.status.busy": "2022-07-13T18:56:06.602816Z",
     "iopub.status.idle": "2022-07-13T18:56:06.623349Z",
     "shell.execute_reply": "2022-07-13T18:56:06.622264Z"
    },
    "papermill": {
     "duration": 0.028808,
     "end_time": "2022-07-13T18:56:06.626135",
     "exception": false,
     "start_time": "2022-07-13T18:56:06.597327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_new_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf98c58",
   "metadata": {
    "papermill": {
     "duration": 0.003537,
     "end_time": "2022-07-13T18:56:06.633609",
     "exception": false,
     "start_time": "2022-07-13T18:56:06.630072",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Load the Image Data\n",
    "\n",
    "Our raw data is divided into a training data directory and a validation data directory. Each of these directories contains subdirectories for urban and rural pictures. TensorFlow offers a powerful tool for handling images organized into directories based on their labels, namely the `ImageDataGenerator`.\n",
    "\n",
    "The process involves two steps with `ImageDataGenerator`. First, we create the generator object in the abstract. We specify applying the ResNet preprocessing function each time it reads an image, and it can also generate additional images through data augmentation.\n",
    "\n",
    "Next, we use the `flow_from_directory` command. We inform it of the data directory, the desired image size, the batch size (number of images to read at a time), and specify that we are classifying data into different categories. A similar approach is employed to set up data reading for the validation set.\n",
    "\n",
    "`ImageDataGenerator` is particularly valuable when dealing with large datasets, as it eliminates the need to store the entire dataset in memory. However, it's also beneficial in scenarios with smaller datasets. Note that these are generators, requiring iteration to extract data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1955bc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T18:56:06.643591Z",
     "iopub.status.busy": "2022-07-13T18:56:06.642491Z",
     "iopub.status.idle": "2022-07-13T18:56:06.872821Z",
     "shell.execute_reply": "2022-07-13T18:56:06.871020Z"
    },
    "papermill": {
     "duration": 0.2384,
     "end_time": "2022-07-13T18:56:06.875641",
     "exception": false,
     "start_time": "2022-07-13T18:56:06.637241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 72 images belonging to 2 classes.\n",
      "Found 20 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "image_size = 224\n",
    "\n",
    "# without data augmentation\n",
    "# data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "# with data augmentation\n",
    "data_generator = ImageDataGenerator(preprocessing_function=preprocess_input, horizontal_flip=True)\n",
    "\n",
    "\n",
    "train_generator = data_generator.flow_from_directory(\n",
    "        './input/urban-and-rural-photos/train',\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=12,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "        './input/urban-and-rural-photos/val',\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=20,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ab38cd",
   "metadata": {
    "papermill": {
     "duration": 0.003752,
     "end_time": "2022-07-13T18:56:06.883922",
     "exception": false,
     "start_time": "2022-07-13T18:56:06.880170",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Fit the Model\n",
    "Let's proceed with fitting the model. The training data is sourced from `train_generator`, while the validation data comes from `validation_generator`. Given that we have 72 training images and load 12 images per batch, we set the number of steps for a single epoch to 6 (`steps_per_epoch=6`). Similarly, there are 20 validation images, and as we load all 20 images in a single step, we use one validation step (validation_steps=1).\n",
    "\n",
    "During the model training, progress updates will be displayed, showcasing the evolution of our loss function and accuracy. The dense layer connections are adjusted as the model refines its understanding of distinguishing between urban and rural photos. Upon completion, the model achieves 76% accuracy on the training data. Subsequently, it evaluates the validation data and attains an accuracy of 90%.\n",
    "\n",
    "It's important to note that this dataset is relatively small, and caution should be exercised when interpreting validation scores derived from such limited data. The intention is to initiate the learning process with small datasets, allowing for quick model training to build foundational experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8d4a4bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T18:56:06.893678Z",
     "iopub.status.busy": "2022-07-13T18:56:06.893257Z",
     "iopub.status.idle": "2022-07-13T18:56:19.625274Z",
     "shell.execute_reply": "2022-07-13T18:56:19.623373Z"
    },
    "papermill": {
     "duration": 12.740164,
     "end_time": "2022-07-13T18:56:19.627960",
     "exception": false,
     "start_time": "2022-07-13T18:56:06.887796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 6s 692ms/step - loss: 0.3363 - accuracy: 0.8611 - val_loss: 0.1408 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1bb235a5ac0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_new_model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=6,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4b2b5e",
   "metadata": {
    "papermill": {
     "duration": 0.004277,
     "end_time": "2022-07-13T18:56:19.637103",
     "exception": false,
     "start_time": "2022-07-13T18:56:19.632826",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Despite the limited size of the training dataset, the achieved accuracy score is remarkably high, considering we trained on only 72 photos. One could easily amass a comparable number of photos using a smartphone, upload them to platforms like [Kaggle Datasets](https://www.kaggle.com/datasets), and construct a highly accurate model capable of distinguishing various subjects of interest.\n",
    "\n",
    "The training process is remarkably swift. Ordinarily, training a neural network can be a time-consuming endeavor, especially when dealing with extensive datasets such as [ImageNet](https://en.wikipedia.org/wiki/ImageNet). This emphasizes the efficiency and effectiveness of transfer learning in accelerating the training process.\n",
    "\n",
    "### Note on Results\n",
    "The displayed validation accuracy may seem notably superior to the training accuracy at this stage, which might initially be perplexing.\n",
    "\n",
    "This difference stems from the fact that the training accuracy is calculated at different intervals as the neural network undergoes refinement (updating the numbers in the convolutions to enhance model accuracy). During the initial encounter with training images, the weights haven't undergone extensive training or improvement yet, influencing the initial training accuracy calculation. These initial results are then averaged into the overall measure.\n",
    "\n",
    "In contrast, validation loss and accuracy metrics are computed ***after*** the model has processed the entire dataset. At this juncture, the network has completed thorough training, leading to the determination of these scores. Although this disparity may be puzzling initially, it is not a significant concern in practice and is typically not a cause for worry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddccbed",
   "metadata": {},
   "source": [
    "### Try Other Pre-trained Model\n",
    "\n",
    "Let's explore another pre-trained model to assess the effectiveness of transfer learning. Xception, highlighted in the previous slide as the most accurate model, boasts a top-1 accuracy of 79% and a top-5 accuracy of 94.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90a06fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def create_model_from_xception():\n",
    "    num_classes = 2\n",
    "\n",
    "    my_new_model = Sequential()\n",
    "    my_new_model.add(Xception(include_top=False, pooling='avg', weights=\"imagenet\"))\n",
    "    my_new_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Say not to train first layer (ResNet) model. It is already trained\n",
    "    my_new_model.layers[0].trainable = False\n",
    "    return my_new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bf731dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 72 images belonging to 2 classes.\n",
      "Found 20 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "image_size = 224\n",
    "\n",
    "# without data augmentation\n",
    "# data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "# with data augmentation\n",
    "data_generator = ImageDataGenerator(preprocessing_function=preprocess_input, horizontal_flip=True)\n",
    "\n",
    "\n",
    "train_generator = data_generator.flow_from_directory(\n",
    "        './input/urban-and-rural-photos/train',\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=12,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "        './input/urban-and-rural-photos/val',\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=20,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78437f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " xception (Functional)       (None, 2048)              20861480  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 4098      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20865578 (79.60 MB)\n",
      "Trainable params: 4098 (16.01 KB)\n",
      "Non-trainable params: 20861480 (79.58 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_new_model = create_model_from_xception()\n",
    "my_new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "731e00bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5b1d155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 5s 592ms/step - loss: 0.6091 - accuracy: 0.7500 - val_loss: 0.4406 - val_accuracy: 0.9500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1bb29aad430>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_new_model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=6,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qlib",
   "language": "python",
   "name": "qlib"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 41.285641,
   "end_time": "2022-07-13T18:56:22.624566",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-13T18:55:41.338925",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
